# Veris-Ecosystem (Backend)

## Project Goal

The objective is to build a multi-step, autonomous investigative agent using Python and LangGraph. The agent's mission is to investigate online information related to a specific query. It operates in a continuous loop: it gathers evidence from various sources, verifies its credibility, analyzes the relationships between findings, and strategically decides whether to dig deeper or conclude the investigation and generate a final report.

## Workflow & Core Files

The agent's logic is broken down into a series of nodes within the `workflow/` directory. The data flows sequentially through these nodes.

### Workflow Diagram


![WhatsApp Image 2025-09-04 at 04 38 23_aa33e0ba](https://github.com/user-attachments/assets/96853cca-bd84-4807-8b33-c264d6cfd127)


### Detailed Workflow Breakdown

The entire process is managed by a central `AgentState` object which acts as the "working memory" for a single investigative cycle. Here is a step-by-step breakdown of the data flow:

1. **Initiation (Cycle Start)**  
   - **Input**: The agent starts with a `current_query` in its `AgentState`. For the first cycle, this is the user's initial query. For subsequent cycles, it's the new query generated by the Refiner.  
   - **Action**: The state is passed to the Collector node.

2. **Collector Node (`workflow/collector.py`)**  
   - **Purpose**: To gather raw data from the internet.  
   - **Process**:  
     - An LLM planner analyzes the `current_query` to select the most appropriate tool (e.g., `search_and_scrape_x` for a Twitter search).  
     - The selected tool is executed, fetching raw data (e.g., tweet text, author info, media URLs).  
     - The raw, often messy, tool output is rigorously cleaned and formatted into a structured list of `EvidenceItem` objects.  
   - **Output**: The `AgentState` is updated with a `new_evidence` list containing the findings.

3. **Verifier Node (`workflow/verifier.py`)**  
   - **Purpose**: To assess the credibility of the new findings.  
   - **Process**:  
     - The node iterates through each `EvidenceItem` in the `new_evidence` list.  
     - For each item, it calls an LLM to act as a threat analyst, assigning a `trust_score` (0.0-1.0) and a `flag_reason` (e.g., "Unverified Claim", "Factual Report").  
     - The results are structured into `VerificationResult` objects.  
   - **Output**: The `AgentState` is updated with a `new_analysis` list.

4. **Graph Node (`workflow/graph.py`)**  
   - **Purpose**: To synthesize all known information into a relationship map.  
   - **Process**:  
     - This node gathers evidence from all previous cycles (from the `investigation_cycles` field in the state) and combines it with the `new_evidence` from the current cycle.  
     - It processes this complete list to identify all unique entities (authors, media) and maps their interactions (mentions, posts).  
   - **Output**: The `AgentState` is updated with a `graph_context` JSON object, which contains a list of all nodes and edges in the investigation so far.

5. **Refiner Node (`workflow/refiner.py`)**  
   - **Purpose**: The strategic decision-making core.  
   - **Process**:  
     - It receives the `graph_context` from the previous step.  
     - An LLM acting as a lead investigator analyzes the graph to identify the most promising leads or determine if the investigation is complete.  
   - **Output**: The `AgentState` is updated with a `next_query`. This field will either contain a new, specific query string (e.g., "Profile the author @SomeUser") or `null` if the decision is to conclude.

6. **Loop or Conclude (Conditional Edge in the final graph)**  
   - **Action**: The final assembled graph will check the `next_query` field.  
     - If `next_query` is a string: The agent prepares for a new cycle. The `new_evidence` and `new_analysis` are archived, the `cycle_id` is incremented, and the `current_query` is updated with the `next_query`. The process then loops back to Step 2.  
     - If `next_query` is `null`: The investigation is complete. The graph proceeds to a final Reporter node.

### Core Files

- **`schema.py`**: The single source of truth for all data structures. It uses Pydantic models to define objects like `EvidenceItem`, `VerificationResult`, and the main `Investigation` object.

- **`workflow/state.py`**: Defines the `AgentState`, a `TypedDict` that acts as the agent's in-memory "working directory" during a LangGraph run.

- **`workflow/collector.py`**:  The data-gathering node. It takes a text query, uses an LLM planner to select the correct tool, executes it, and formats the raw output into clean `EvidenceItem` objects.

- **`workflow/verifier.py`**:  The credibility-analysis node. It examines each new `EvidenceItem` and produces a `VerificationResult`, assigning a trust score and a flag reason.

- **`workflow/graph.py`**:  The synthesis node. It aggregates all evidence collected so far and generates a structured JSON graph of entities (authors, media) and their relationships (mentions, posts).

- **`workflow/refiner.py`**:  The strategic brain. It analyzes the graph from the previous node and decides the next action: either generate a new, targeted query to continue the investigation or conclude if no promising leads exist.

- **`workflow/reporter.py`**: This node is responsible for generating the final summary report when the refiner decides to conclude. It is well-built and robust but has not yet been added to the final graph.
